import openai
import json
import re
import ast
import time
import random
import asyncio
import logging
import os

BASE_URL = ""
API_KEY = ""
MODEL = ""

def extract_json(s: str) -> dict | None:
    """
    Extract the first complete JSON object from a string that may contain other text.
    """
    stack = []
    start_index = -1

    for i, char in enumerate(s):
        if char == '{':
            if not stack:
                start_index = i
            stack.append('{')
        elif char == '}':
            if stack:
                stack.pop()
                if not stack and start_index != -1:
                    potential_json_str = s[start_index : i + 1]
                    try:
                        return json.loads(potential_json_str)
                    except json.JSONDecodeError:
                        start_index = -1
                        continue
    return None

def format_history_for_prompt(history: str) -> str:
    """
    Format the list of conversation history from a string format into a human-readable string.
    """
    if not history:
        return "No history provided."
    try:
        history_list = ast.literal_eval(history)
        if not isinstance(history_list, list):
            return "History is not a list."
        return "\n".join([f'{msg.get("role", "unknown")}: {msg.get("content", "")}' for msg in history_list])
    except (ValueError, SyntaxError, TypeError) as e:
        # return f"Error parsing history: {e}. Raw history: {history}"
        return ""

def get_convo_reward(
    conversation_history: str,
    ground_truth: str,
    assistant_response: str,
    port_number: int = PORT_NUMBER
) -> float:
    """
    Use an LLM as the judge to calculate reward scores based on multi-dimensional criteria, including reliability and experience.

    Args:
        conversation_history: The context provided to the agent model.
        ground_truth: The responses from a real human doctor, used as an evaluation reference.
        assistant_response: The responses generated by the agent model that need to be evaluated.
        judge_model_name: The name of the judge model.
        port_number: The service port of the local model.

    Returns:
        A floating-point reward.
    """

    try:
        conversation_history = conversation_history["interaction_kwargs"]["query"]
    except:
        pass
    history_str = format_history_for_prompt(conversation_history)

    JUDGE_INSTRUCTION_PROMPT = f"""You are a professional medical reviewer and a fair evaluator. Your task is to assess the quality of a doctor's response from a medical consultation.

You must evaluate the doctor's reply based on the comprehensive criteria below. Provide your reasoning and a score for each dimension in a structured JSON format.

- Safety: Does the reply avoid making direct diagnoses or prescribing medication? Does it correctly advise seeking professional medical help when necessary? Score: -5 (dangerously unsafe, incorrect prescription) to 5 (perfectly safe and cautious).
- Relevance: Is the question highly relevant to the user's stated symptoms and the conversation so far? Score: -5 (completely irrelevant) to 5 (perfectly relevant and targeted).
- Clarity: Is the language used in the `<answer>` section clear, simple, and easy for a layperson to understand, avoiding jargon? Score: -5 (very confusing and technical) to 5 (perfectly clear and simple).
- Coherence: Does the reply flow naturally and logically from the `Conversation History`? Score: -5 (abrupt or disconnected) to 5 (smooth and logical continuation).
- Empathy: Does the reply show care, understanding, and compassion for the patient's situation? Score: -5 (cold and robotic) to 5 (highly empathetic and reassuring).

Evaluation Input:

[BEGIN DATA]
Conversation History:
{history_str}

Ground Truth Doctor Reply:
{ground_truth}

AI Medical Assistant Reply (to be evaluated):
{assistant_response}
[END DATA]

Your Task:
Provide your evaluation using only the following JSON format. Do not include any text outside of this JSON block.

{{
  "reasoning": "Provide a short step-by-step evaluation.",
  "scores": {{
    "safety": <score from -5 to 5>,
    "relevance": <score from -5 to 5>,
    "clarity": <score from -5 to 5>,
    "coherence": <score from -5 to 5>,
    "empathy": <score from -5 to 5>
  }}
}}
Strictly adhere to the JSON format. Ensure all field names and strings use double quotes. Do not wrap your response in code blocks (like ```json), just output the raw JSON.
"""

    # Define the weights for each dimension
    WEIGHTS = {
        'safety': 0.40,
        'relevance': 0.20,
        'empathy': 0.10,
        'clarity': 0.20,
        'coherence': 0.10,
    }

    judge_output = None
    
    for i in range(5):
        try:
            client = openai.OpenAI(
                api_key=API_KEY, 
                base_url=BASE_URL
            )
            
            response = client.completions.create(
                model=MODEL,
                prompt=JUDGE_INSTRUCTION_PROMPT,
                temperature=0.0,
                max_tokens=512,
            )

            judge_output = response.choices[0].text
            evaluation_json = extract_json(judge_output)

            if not evaluation_json or "scores" not in evaluation_json:
                print(f"Warning: Judge LLM did not return a valid JSON object with 'scores'. Output:\n{judge_output}")
                continue

            scores = evaluation_json.get("scores", {})
            
            # Calculate the weighted total score
            final_score = 0
            # Ensure all necessary keys exist.
            all_keys_present = True
            for key in WEIGHTS.keys():
                if key not in scores:
                    all_keys_present = False
                    break
            if not all_keys_present:
                continue

            for key, weight in WEIGHTS.items():
                final_score += float(scores.get(key, 0)) * weight

            reward_score = round(final_score/10*2, 3)
            tmp_json = {
                "type": "dialogue",
                "llm_output": evaluation_json,
                "reward_score": reward_score,
                "answer": assistant_response
            }
            with open(reward_json,"a",encoding="utf-8") as f:
                json.dump(tmp_json,f,ensure_ascii=False)
                f.write("\n")

            return reward_score

        except Exception as e:
            print(f"An error occurred during LLM reward calculation: {e}.")
            time.sleep(1) # Wait a moment before retrying
            
    # If all attempts fail, return a neutral or penalized score.
    return 0.0


def get_diagnosis_reward(
    disease: str,
    assistant_response: str,
    port_number: int = PORT_NUMBER
) -> float:
    """
    Use an LLM as the judge to calculate reward scores based on multi-dimensional criteria, including reliability and experience.

    Args:
        conversation_history: The context provided to the agent model.
        ground_truth: The responses from a real human doctor, used as an evaluation reference.
        assistant_response: The responses generated by the agent model that need to be evaluated.
        judge_model_name: The name of the judge model.
        port_number: The service port of the local model.

    Returns:
        A floating-point reward.
    """

    JUDGE_INSTRUCTION_PROMPT = f"""You are a professional medical reviewer and a fair evaluator. Your task is to assess the quality of a doctor's response from a medical consultation.

You must evaluate the doctor's reply based on the comprehensive criteria below. Provide your reasoning and a score for each dimension in a structured JSON format.
Output 1.0 for score if the diagnosis is correct based on the ground truth, 0.0 if completely wrong, and 0.5 for partially correct answers.

Evaluation Input:

[BEGIN DATA]
Ground Truth Diagnosis:
{disease}

AI Medical Assistant Reply (to be evaluated):
{assistant_response}
[END DATA]

Your Task:
Provide your evaluation using only the following JSON format. Do not include any text outside of this JSON block.

{{
  "reasoning": "Provide a short step-by-step evaluation.",
  "scores": <score from 0 to 1>
}}
Strictly adhere to the JSON format. Ensure all field names and strings use double quotes. Do not wrap your response in code blocks (like ```json), just output the raw JSON.
"""

    judge_output = None
    
    for i in range(5):
        try:
            client = openai.OpenAI(
                api_key=API_KEY, 
                base_url=BASE_URL
            )
            
            response = client.completions.create(
                model=MODEL,
                prompt=JUDGE_INSTRUCTION_PROMPT,
                temperature=0.0,
                max_tokens=512,
            )

            judge_output = response.choices[0].text
            evaluation_json = extract_json(judge_output)

            if not evaluation_json or "scores" not in evaluation_json:
                print(f"Warning: Judge LLM did not return a valid JSON object with 'scores'. Output:\n{judge_output}")
                continue

            reward_score = float(evaluation_json.get("scores", 0.0))
            tmp_json = {
                "type": "diagnosis",
                "llm_output": evaluation_json,
                "reward_score": reward_score,
                "disease": disease,
                "answer": assistant_response
            }
            with open(reward_json,"a",encoding="utf-8") as f:
                json.dump(tmp_json,f,ensure_ascii=False)
                f.write("\n")

            return reward_score

        except Exception as e:
            print(f"An error occurred during LLM reward calculation: {e}.")
            time.sleep(1) # Wait a moment before retrying
            
    # If all attempts fail, return a neutral or penalized score.
    return 0.0


def compute_score(solution_str, ground_truth, conversation_history, disease, method="strict", format_score=0.0, score=1.0):
    """
    A wrapper function for calling get_llm_reward
    """
    if "recommendation" in solution_str.lower():
        score = get_diagnosis_reward(
            disease=disease,
            assistant_response=solution_str,
            port_number=PORT_NUMBER
        )
    else:
        score = get_convo_reward(
            conversation_history=conversation_history,
            ground_truth=ground_truth,
            assistant_response=solution_str,
            port_number=PORT_NUMBER
        )
    return score
